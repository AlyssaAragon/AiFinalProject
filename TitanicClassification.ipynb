{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow --upgrade\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G7aqOrz10WXf",
        "outputId": "ef5a1eb5-2980-4355-f15d-1c8e4795654b",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.18.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.25.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.68.1)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.5.0)\n",
            "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.26.4)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.12.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.5.0->tensorflow) (0.13.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.8.30)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import tensorflow as tf\n",
        "\n",
        "import seaborn as sns\n",
        "import joblib\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "Suj8eMiBzrdu"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#we are gonna do logistic regression, xgboost, a neural network, xgboost, and random forest\n",
        "\n",
        "train_data = pd.read_csv('train.csv')\n",
        "test_data = pd.read_csv('test.csv')\n",
        "\n",
        "print(train_data.head())\n",
        "print(train_data.info())\n",
        "print(test_data.head())\n"
      ],
      "metadata": {
        "id": "IyXYpiKNM6g6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "186aab10-5d6c-46b8-9734-af621b202990",
        "collapsed": true
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   PassengerId  Survived  Pclass  \\\n",
            "0            1         0       3   \n",
            "1            2         1       1   \n",
            "2            3         1       3   \n",
            "3            4         1       1   \n",
            "4            5         0       3   \n",
            "\n",
            "                                                Name     Sex   Age  SibSp  \\\n",
            "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
            "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
            "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
            "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
            "4                           Allen, Mr. William Henry    male  35.0      0   \n",
            "\n",
            "   Parch            Ticket     Fare Cabin Embarked  \n",
            "0      0         A/5 21171   7.2500   NaN        S  \n",
            "1      0          PC 17599  71.2833   C85        C  \n",
            "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
            "3      0            113803  53.1000  C123        S  \n",
            "4      0            373450   8.0500   NaN        S  \n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 891 entries, 0 to 890\n",
            "Data columns (total 12 columns):\n",
            " #   Column       Non-Null Count  Dtype  \n",
            "---  ------       --------------  -----  \n",
            " 0   PassengerId  891 non-null    int64  \n",
            " 1   Survived     891 non-null    int64  \n",
            " 2   Pclass       891 non-null    int64  \n",
            " 3   Name         891 non-null    object \n",
            " 4   Sex          891 non-null    object \n",
            " 5   Age          714 non-null    float64\n",
            " 6   SibSp        891 non-null    int64  \n",
            " 7   Parch        891 non-null    int64  \n",
            " 8   Ticket       891 non-null    object \n",
            " 9   Fare         891 non-null    float64\n",
            " 10  Cabin        204 non-null    object \n",
            " 11  Embarked     889 non-null    object \n",
            "dtypes: float64(2), int64(5), object(5)\n",
            "memory usage: 83.7+ KB\n",
            "None\n",
            "   PassengerId  Pclass                                          Name     Sex  \\\n",
            "0          892       3                              Kelly, Mr. James    male   \n",
            "1          893       3              Wilkes, Mrs. James (Ellen Needs)  female   \n",
            "2          894       2                     Myles, Mr. Thomas Francis    male   \n",
            "3          895       3                              Wirz, Mr. Albert    male   \n",
            "4          896       3  Hirvonen, Mrs. Alexander (Helga E Lindqvist)  female   \n",
            "\n",
            "    Age  SibSp  Parch   Ticket     Fare Cabin Embarked  \n",
            "0  34.5      0      0   330911   7.8292   NaN        Q  \n",
            "1  47.0      1      0   363272   7.0000   NaN        S  \n",
            "2  62.0      0      0   240276   9.6875   NaN        Q  \n",
            "3  27.0      0      0   315154   8.6625   NaN        S  \n",
            "4  22.0      1      1  3101298  12.2875   NaN        S  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cjoBvcwMw8Kl",
        "outputId": "86438871-c3a6-4b18-be32-79ca1dacb9a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess the data\n",
        "train_data = pd.read_csv('train.csv')\n",
        "test_data = pd.read_csv('test.csv')\n",
        "\n",
        "train_data['Age'] = train_data['Age'].fillna(train_data['Age'].median())\n",
        "train_data['Embarked'] = train_data['Embarked'].fillna(train_data['Embarked'].mode()[0])\n",
        "train_data['Fare'] = train_data['Fare'].fillna(train_data['Fare'].median())\n",
        "\n",
        "le_sex = LabelEncoder()\n",
        "le_embarked = LabelEncoder()\n",
        "\n",
        "train_data['Sex'] = le_sex.fit_transform(train_data['Sex'])\n",
        "train_data['Embarked'] = le_embarked.fit_transform(train_data['Embarked'])\n",
        "\n",
        "joblib.dump(le_sex, 'le_sex.pkl')\n",
        "joblib.dump(le_embarked, 'le_embarked.pkl')\n",
        "\n",
        "train_data = train_data.drop(['PassengerId', 'Name', 'Ticket', 'Cabin'], axis=1)\n",
        "\n",
        "X = train_data.drop('Survived', axis=1)\n",
        "y = train_data['Survived']"
      ],
      "metadata": {
        "id": "bmg_Wlk81EF2"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Scale data\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "d3w_39nN22WP"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Logistic Regression\n",
        "log_reg_model = LogisticRegression()\n",
        "log_reg_model.fit(X_train, y_train)\n",
        "log_reg_preds = log_reg_model.predict(X_test)\n",
        "print(\"Logistic Regression Accuracy:\", accuracy_score(y_test, log_reg_preds))"
      ],
      "metadata": {
        "id": "0N_3sXwa3I_5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52f8ab3c-b5b3-4e0d-859b-92c287b55e01"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression Accuracy: 0.8044692737430168\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# XGBoost\n",
        "xgb_model = XGBClassifier(eval_metric='logloss')\n",
        "xgb_model.fit(X_train, y_train)\n",
        "xgb_preds = xgb_model.predict(X_test)\n",
        "print(\"XGBoost Accuracy:\", accuracy_score(y_test, xgb_preds))"
      ],
      "metadata": {
        "id": "VR2NeTHT263g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "311ab8ae-f996-4e6a-d5ae-0b6a0eec6ffa"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGBoost Accuracy: 0.7877094972067039\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Neural Network\n",
        "y_train_cat = to_categorical(y_train)\n",
        "y_test_cat = to_categorical(y_test)\n",
        "\n",
        "nn_model = Sequential([\n",
        "    Dense(32, activation='relu', input_dim=X_train.shape[1]),\n",
        "    Dropout(0.3),\n",
        "    Dense(16, activation='relu'),\n",
        "    Dropout(0.3),\n",
        "    Dense(2, activation='softmax')\n",
        "])\n",
        "\n",
        "nn_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "nn_model.fit(X_train, y_train_cat, epochs=20, batch_size=32, validation_split=0.2)\n",
        "\n",
        "nn_loss, nn_accuracy = nn_model.evaluate(X_test, y_test_cat)\n",
        "print(\"Neural Network Accuracy:\", nn_accuracy)\n",
        "\n",
        "joblib.dump(log_reg_model, 'log_reg_model.pkl')\n",
        "joblib.dump(xgb_model, 'xgb_model.pkl')\n",
        "nn_model.save('nn_model.h5')"
      ],
      "metadata": {
        "id": "jKbsjTJ33NWh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0632682-732d-4a7f-a027-6d1057d5137d"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 41ms/step - accuracy: 0.5394 - loss: 0.7524 - val_accuracy: 0.7343 - val_loss: 0.6407\n",
            "Epoch 2/20\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.6400 - loss: 0.6541 - val_accuracy: 0.7902 - val_loss: 0.5886\n",
            "Epoch 3/20\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.6925 - loss: 0.6302 - val_accuracy: 0.7972 - val_loss: 0.5520\n",
            "Epoch 4/20\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7273 - loss: 0.5765 - val_accuracy: 0.7762 - val_loss: 0.5233\n",
            "Epoch 5/20\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7388 - loss: 0.5743 - val_accuracy: 0.7972 - val_loss: 0.4953\n",
            "Epoch 6/20\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.7485 - loss: 0.5577 - val_accuracy: 0.7972 - val_loss: 0.4748\n",
            "Epoch 7/20\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7553 - loss: 0.5518 - val_accuracy: 0.8042 - val_loss: 0.4587\n",
            "Epoch 8/20\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7356 - loss: 0.5501 - val_accuracy: 0.8112 - val_loss: 0.4467\n",
            "Epoch 9/20\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7782 - loss: 0.4863 - val_accuracy: 0.8112 - val_loss: 0.4351\n",
            "Epoch 10/20\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7889 - loss: 0.5143 - val_accuracy: 0.8112 - val_loss: 0.4275\n",
            "Epoch 11/20\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7887 - loss: 0.5478 - val_accuracy: 0.8112 - val_loss: 0.4214\n",
            "Epoch 12/20\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7753 - loss: 0.5237 - val_accuracy: 0.8112 - val_loss: 0.4172\n",
            "Epoch 13/20\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7615 - loss: 0.5537 - val_accuracy: 0.8112 - val_loss: 0.4149\n",
            "Epoch 14/20\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7785 - loss: 0.5068 - val_accuracy: 0.8112 - val_loss: 0.4139\n",
            "Epoch 15/20\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7582 - loss: 0.5153 - val_accuracy: 0.8112 - val_loss: 0.4132\n",
            "Epoch 16/20\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7843 - loss: 0.4631 - val_accuracy: 0.8112 - val_loss: 0.4110\n",
            "Epoch 17/20\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7867 - loss: 0.4756 - val_accuracy: 0.8112 - val_loss: 0.4057\n",
            "Epoch 18/20\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7874 - loss: 0.4725 - val_accuracy: 0.8112 - val_loss: 0.4050\n",
            "Epoch 19/20\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7900 - loss: 0.4821 - val_accuracy: 0.8112 - val_loss: 0.4045\n",
            "Epoch 20/20\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7475 - loss: 0.5377 - val_accuracy: 0.8182 - val_loss: 0.4053\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.7628 - loss: 0.4819\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Neural Network Accuracy: 0.7444444298744202\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "# K-Nearest Neighbors\n",
        "knn_model = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_model.fit(X_train, y_train)\n",
        "knn_preds = knn_model.predict(X_test)\n",
        "print(\"K-Nearest Neighbors Accuracy:\", accuracy_score(y_test, knn_preds))\n",
        "\n",
        "joblib.dump(knn_model, 'knn_model.pkl')\n",
        "\n",
        "# Random Forest\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "rf_preds = rf_model.predict(X_test)\n",
        "print(\"Random Forest Accuracy:\", accuracy_score(y_test, rf_preds))\n",
        "\n",
        "joblib.dump(rf_model, 'rf_model.pkl')"
      ],
      "metadata": {
        "id": "cMoty7fTvasX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bfabde2e-1c45-4008-8a00-9d78f91875a7"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "K-Nearest Neighbors Accuracy: 0.8044692737430168\n",
            "Random Forest Accuracy: 0.8212290502793296\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['rf_model.pkl']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def ensemble_predict(xgb_model, log_reg_model, nn_model, knn_model, rf_model, input_features, weights=[1, 1, 1, 3, 3]):\n",
        "    \"\"\"\n",
        "    Function to ensemble predictions by averaging the predicted probabilities of multiple models, giving more weight to the more complex models.\n",
        "    \"\"\"\n",
        "    xgb_prob = xgb_model.predict_proba(input_features)[:, 1][0]\n",
        "    log_reg_prob = log_reg_model.predict_proba(input_features)[:, 1][0]\n",
        "    nn_prob = nn_model.predict(input_features)[0][1]\n",
        "    knn_prob = knn_model.predict_proba(input_features)[:, 1][0]\n",
        "    rf_prob = rf_model.predict_proba(input_features)[:, 1][0]\n",
        "\n",
        "    total_weight = sum(weights)\n",
        "    weighted_avg_prob = (\n",
        "        xgb_prob * weights[0] +\n",
        "        log_reg_prob * weights[1] +\n",
        "        nn_prob * weights[2] +\n",
        "        knn_prob * weights[3] +\n",
        "        rf_prob * weights[4]\n",
        "    ) / total_weight\n",
        "\n",
        "    return weighted_avg_prob\n",
        "\n",
        "def predict_survival(xgb_model, log_reg_model, nn_model, knn_model, rf_model):\n",
        "    print(\"Enter your details to predict survival:\")\n",
        "    pclass = int(input(\"Passenger Class (1 = 1st, 2 = 2nd, 3 = 3rd): \"))\n",
        "    sex = input(\"Sex (male/female): \").strip().lower()\n",
        "    age = float(input(\"Age: \"))\n",
        "    sibsp = int(input(\"Number of Siblings/Spouses Aboard: \"))\n",
        "    parch = int(input(\"Number of Parents/Children Aboard: \"))\n",
        "    fare = float(input(\"Ticket Fare: \"))\n",
        "    embarked = input(\"Port of Embarkation (C = Cherbourg, Q = Queenstown, S = Southampton): \").strip().upper()\n",
        "\n",
        "    sex_encoded = le_sex.transform([sex])[0]\n",
        "    embarked_encoded = le_embarked.transform([embarked])[0]\n",
        "    input_features = np.array([[pclass, sex_encoded, age, sibsp, parch, fare, embarked_encoded]])\n",
        "\n",
        "    xgb_prob = xgb_model.predict_proba(input_features)[:, 1][0]\n",
        "    log_reg_prob = log_reg_model.predict_proba(input_features)[:, 1][0]\n",
        "    nn_prob = nn_model.predict(input_features)[0][1]\n",
        "    knn_prob = knn_model.predict_proba(input_features)[:, 1][0]\n",
        "    rf_prob = rf_model.predict_proba(input_features)[:, 1][0]\n",
        "\n",
        "    final_prob = ensemble_predict(xgb_model, log_reg_model, nn_model, knn_model, rf_model, input_features)\n",
        "\n",
        "    print(f\"XGBoost Model Probability: {xgb_prob * 100:.2f}%\")\n",
        "    print(f\"Logistic Regression Model Probability: {log_reg_prob * 100:.2f}%\")\n",
        "    print(f\"Neural Network Model Probability: {nn_prob * 100:.2f}%\")\n",
        "    print(f\"K-Nearest Neighbors Model Probability: {knn_prob * 100:.2f}%\")\n",
        "    print(f\"Random Forest Model Probability: {rf_prob * 100:.2f}%\")\n",
        "    print(f\"Final Survival Probability: {final_prob * 100:.2f}%\")"
      ],
      "metadata": {
        "id": "DIrOCMNo3oCt"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "import numpy as np\n",
        "\n",
        "log_reg_model = joblib.load('log_reg_model.pkl')\n",
        "xgb_model = joblib.load('xgb_model.pkl')\n",
        "nn_model = tf.keras.models.load_model('nn_model.h5')\n",
        "knn_model = joblib.load('knn_model.pkl')\n",
        "rf_model = joblib.load('rf_model.pkl')\n",
        "\n",
        "def print_metrics(y_test, predictions, model_name):\n",
        "    accuracy = accuracy_score(y_test, predictions)\n",
        "    precision = precision_score(y_test, predictions)\n",
        "    recall = recall_score(y_test, predictions)\n",
        "    f1 = f1_score(y_test, predictions)\n",
        "    print(f\"{model_name} - Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}\")\n",
        "\n",
        "log_reg_preds = log_reg_model.predict(X_test)\n",
        "print_metrics(y_test, log_reg_preds, \"Logistic Regression\")\n",
        "\n",
        "xgb_preds = xgb_model.predict(X_test)\n",
        "print_metrics(y_test, xgb_preds, \"XGBoost\")\n",
        "\n",
        "nn_preds = np.argmax(nn_model.predict(X_test), axis=-1)\n",
        "print_metrics(y_test, nn_preds, \"Neural Network\")\n",
        "\n",
        "knn_preds = knn_model.predict(X_test)\n",
        "print_metrics(y_test, knn_preds, \"K-Nearest Neighbors\")\n",
        "\n",
        "rf_preds = rf_model.predict(X_test)\n",
        "print_metrics(y_test, rf_preds, \"Random Forest\")\n",
        "\n",
        "predict_survival(xgb_model, log_reg_model, nn_model, knn_model, rf_model)"
      ],
      "metadata": {
        "id": "B5j70dOQ32J7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48b31090-f07f-4b1b-89d8-d2f610160713"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression - Accuracy: 0.7333, Precision: 0.6471, Recall: 0.6471, F1 Score: 0.6471\n",
            "XGBoost - Accuracy: 0.7667, Precision: 0.6757, Recall: 0.7353, F1 Score: 0.7042\n",
            "\r\u001b[1m1/3\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 75ms/step"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 34 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7ad84ef8d630> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
            "Neural Network - Accuracy: 0.7444, Precision: 0.6571, Recall: 0.6765, F1 Score: 0.6667\n",
            "K-Nearest Neighbors - Accuracy: 0.7444, Precision: 0.6774, Recall: 0.6176, F1 Score: 0.6462\n",
            "Random Forest - Accuracy: 0.7778, Precision: 0.7333, Recall: 0.6471, F1 Score: 0.6875\n",
            "Enter your details to predict survival:\n",
            "Passenger Class (1 = 1st, 2 = 2nd, 3 = 3rd): 1\n",
            "Sex (male/female): female\n",
            "Age: 12\n",
            "Number of Siblings/Spouses Aboard: 1\n",
            "Number of Parents/Children Aboard: 1\n",
            "Ticket Fare: 20\n",
            "Port of Embarkation (C = Cherbourg, Q = Queenstown, S = Southampton): S\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "XGBoost Model Probability: 50.11%\n",
            "Logistic Regression Model Probability: 1.13%\n",
            "Neural Network Model Probability: 77.79%\n",
            "K-Nearest Neighbors Model Probability: 80.00%\n",
            "Random Forest Model Probability: 25.67%\n",
            "Final Survival Probability: 49.56%\n"
          ]
        }
      ]
    }
  ]
}